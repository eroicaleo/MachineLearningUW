---
title: "LectureNotesCourse1: Machine Learning Foundations: A Case Study Approach"
author: "Yang Ge"
date: "September 29, 2015"
output:
  html_document:
    keep_md: true
    toc: true
---

# Week1

## Getting started with Python and the IPython Notebook

To install ipython note book on my Ubuntu 15.04, I followed link
[here](http://askubuntu.com/questions/554129/problem-with-installing-ipython-on-ubuntu-14-04-lts)
and discussion forum thread "Hints to install Ipython notebook on Ubuntu 14.04"
[here](https://www.coursera.org/learn/ml-foundations/module/jw04R/discussions/0NmDpWOZEeWNhRIQkbjhXw)

```bash
$ sudo apt-get install ipython ipython-notebook
# If you want to use python 3, I guess you could do
# sudo apt-get install ipython3 ipython3-notebook

# I also need to do this step:
$ sudo easy_install jsonschema

# To start it
$ ipython notebook
```

Python plus wiki pages is **_IPython notebook_**.

* To create new notebook, click "New" -> "Python 2".
* To change the cell from "code" to "markdown", click "Cell" -> "Celltype" -> "Markdown".
Or use the hotkey `ctrl-m m`
* To run the current code block: "Ctrl"+"Enter". To run and create new: "SHIFT"+"Enter".

According to a recent survey "python is the language for data science".

## Getting started with GraphLab Create

To install the GraphLab Create in my Ubuntu 15.04:

```base
# Install the virtualenv
$ sudo apt-get install python-virtualenv

# Create a virtual environment named e.g. dato-env
$ virtualenv dato-env

# Activate the virtual environment
$ source dato-env/bin/activate

# Make sure pip is up to date
$ pip install --upgrade pip

# Install your licensed copy of GraphLab Create
pip install --upgrade --no-cache-dir https://get.dato.com/GraphLab-Create/1.6.1/<email>/<product_key>/GraphLab-Create-License.tar.gz
```

But eventually, I give up because it can only be installed on 64-bit machine.

**Basic command**

`import graphlab`, `graphlab.SFrame("filename")`
`sf.head()`, `sf.tail()`

Canvas command: `sf.show()` is mad cool!

It opens in new tab in the browser, we can also do this:
```python
graphlab.canvas.set_target('ipynb')
sf['age'].show(view='Categorical')
```

**Interacting with columns in SFrame**
```python
sf['Country']
sf['age'].mean()
sf['Full Name'] = sf['First Name'] + sf['Last Name']
sf['age'] * sf['age']
```

**Using .apply() for data transformation**

```python
sf['Country'].apply(transform_country)
```

# Week 2

Predict house price by similar houses.
The problem is that we throw out the info of all other houses.
We would like to leverage all the information we can to come up with good
predictions.

## Linear regression modeling

$$
f(x) = w_0 + w_1 x
$$

$w_0$ is intercept and $w_1$ is slope.
Redidual sum of squares (*RSS*).
Best model is called $\hat{w} = (\hat{w_0}, \hat{w_1})$

## Adding higer order effects

$$
f(x) = w_0 + w_1 x + w_2 x^2
$$

If we go higer, like 13 order, we might overfit the data.

## Evaluating regression models

We want a model generalized well for new data.

pipeline:

Training data -> feature extraction -> ML model

# Week 3 Classification modeling

Input is sentence from review, it goes to "classifier model", output is predicted class.
output $y$ can have more than 2 categories.

* Resteraut review
* Another example is spam filtering, input is "text of email, IP, sender"
* Image classification, different kinds of dogs.
* Personalized medical diagnosis
* Reading mind, take a image of brain

## Linear classifier

* Simple threshold classifier: Count positive and negative word in a sentence.
* Limitation:
    * how do we get list of +/- words?
    * Words have different degree of sentiments.
    * single words are not enough: good and not good.

* A linear classifier will learn a weight for each word.

**simple linear classifier**

score(x) = Weighted count of words in sentence.
if score(x) > 0, then positive. Otherwise negative.

**Decision boundaries**

## Evaluating classification models

error = (# of mistakes) / total # of sentence

Is there class imbalance, how does it compare to random guess and majority vote?

 False positive, false negative

### Learning curve

Even with infinite data, the test error won't be 0, which is called bias of model

Complex model has less bias, but needs more training data.

How confident is your prediction. (P(y|x))

# Week 4 Clustering and Similarity: Retrieving Documents
# Week 4.1 Algorithms for retrieval and measuring similarity of documents

Group related clusters.

## What is document retrieve task?

**Document Retrieval**

* Currently reading article you like
* Automatically retrieve articles might be of interest to you

**Challenges**

* How do we measure similarity?
* How do we search over articles?

## Word count representation for measuring similarity

* Bag of words model
    * Ignore order of words
    * Count # of instances of each word in vocabulary

"Carlos calls the sport futbol. Emily calls the sport soccer."

* Build a very long sparse vector that counts the No. of words
  that we see in this document.

**Measuring similarity**

* Do the inner product (elementwise product) of 2 word count vectors.

**Issue with word counts - Doc length**

* If we double the document, the similarity will be 4X.
* Bias is very strong towards long document.

**Solution = normalize**

* normalized vector = $\frac{v}{\begin{Vmatrix}v\end{Vmatrix}}$

## Prioritizing important words with tf-idf

* We really want to emphasize the important words in a document.

**Issues with word counts - Rare words**

* Common words in doc: "the", "player", "field", "goal".
* Dominate rare words like: "futbol", "Messi".
    * Those often are the ones that are really relevent in describing
      what is unique about this article

**Document Frequency**

* Rare words: appears infrequent in the corpus.
* Emphasize words appearing in few docs.
* Discount word $w$ based on # of docs containing $w$ in corpus.

**Important words**

* What characterize an important word?
    * Appears frequently in doc (common locally)
    * Appears rarely in corpus (rare globally)
* Important words is some trade-off between local frequency and global rarity.

## Calculating tf-idf vectors

* Term frequency - inverse document frequency.
* Term frequency: same as word count vector
* Inverse document frequency:

$$
log \frac{\text{# docs}}{1 + #docs using word}
$$

* For frequent words, the idf $\approx 0$
    * very very strong downweighting
* For rare words, the idf is large/small/non-zero.
* Then we multiply tf and idf together.

## Retrieving similar documents using nearest neighbor search

* Need to specify distance metric.

**1 Nearest neighbor**

* input query article
* output: most similar article
* Algorithm: linear search

**1 Nearest neighbor**

* input query article
* output: list of k similar articles
* Algorithm: Priority Queue

# Week 4.2 Clustering models and algorithms

## Clustering documents task overview

**Structure documents by topic**

* Goal: discover groups (clusters) of related articles.
* Training set of labeled docs.
* Multiclass classification problem
    * Have a bunch of labels
    * Want to classify which class an article belongs to
* Supervised learning problem.

## Clustering documents: An unsupervised learning task

**Clustering**

* No labels are provided
* Want to uncover cluster structure
* Input: docs as vectors
* Output: cluster labels
* We are going to associate some class label with the document.

**What defines a cluster**

* Cluster defined by center and shape/spread
* Assign obeservation (doc) to cluster (topic label)
    * score under cluster is higher than others
    * Often, just more similar to assigned cluster center than other cluster
    centers

## k-means: A clustering algorithm

**k-means**

0. Initialize cluster centers
1. Assign observations to closest cluster center
2. Revise cluster centers as mean of assigned observations
3. Repeat 1 + 2 until convergence.

## Other examples of clustering

* Clustering images
* Group patients by medical condition
* Products on Amazon
    * Discover product categories from purchase history
    * Discover groups of users
* Structuring web search results
    * Search term can have multiple meanings
    * Example: "cardinal"
* Discovering similar neighborhoods
    * Task1: Estimate price at a small regional level
    * Challenge: only a few sales in each region per month
    * Solution: cluster regions with similar trends and share information within
      a cluster
    * Task2: Forecast violent crimes to better task police
    * Again cluster regions and share information

# Week 4.3 Summary of clustering and similarity

## Clustering and similarity ML block diagram

* Training data: document id, document text table
* Feature extraction: tf-idf representation
* Machine learning model: clustering, output $\hat{y}$ is estimated cluster label.
* ML Algorithm: k-means clustering, output $\hat{w}$ cluster centers
* Quality Measure: distances to cluster centers
